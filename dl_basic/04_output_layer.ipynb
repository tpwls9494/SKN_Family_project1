{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 소프트맥스 오버플로우 방지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan nan nan]\n",
      "[0.09003057 0.24472847 0.66524096]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kp/5_0zhw410tzf9tkb2lt76sw40000gn/T/ipykernel_4617/300004869.py:4: RuntimeWarning: overflow encountered in exp\n",
      "  exp_x = np.exp(x)\n",
      "/var/folders/kp/5_0zhw410tzf9tkb2lt76sw40000gn/T/ipykernel_4617/300004869.py:5: RuntimeWarning: invalid value encountered in divide\n",
      "  return exp_x / np.sum(exp_x)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    return exp_x / np.sum(exp_x)\n",
    "\n",
    "def stable_softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x))\n",
    "    return exp_x / np.sum(exp_x)\n",
    "\n",
    "x = np.array([1000, 1001, 1002])\n",
    "\n",
    "print(softmax(x))\n",
    "print(stable_softmax(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- pytouch 라이브러리 함수 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6590, 0.2424, 0.0986])\n",
      "tensor([0.8808, 0.7311, 0.5250])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "x = torch.tensor([2.0, 1.0, 0.1], dtype=torch.float32)\n",
    "\n",
    "softmax_output = F.softmax(x, dim=0)    # dim: softmax를 적용할 축\n",
    "print(softmax_output)\n",
    "\n",
    "sigmoid_output = torch.sigmoid(x)\n",
    "print(sigmoid_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5731444358825684\n",
      "1.54547917842865\n",
      "1.5180859565734863\n",
      "1.490976095199585\n",
      "1.464160442352295\n",
      "1.4376498460769653\n",
      "1.411454439163208\n",
      "1.385584831237793\n",
      "1.360050916671753\n",
      "1.334862232208252\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 간단한 다중 클래스 분류 모델 정의 (입력 값: 5개, 출력: 3 클래스)\n",
    "class SimpleMultiClassModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleMultiClassModel, self).__init__()\n",
    "        self.fc = nn.Linear(5, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = SimpleMultiClassModel()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# 데이터 생성\n",
    "inputs = torch.randn(4, 5)\n",
    "labels = torch.tensor([0, 2, 1, 0])\n",
    "\n",
    "for _ in range(10):\n",
    "    outputs = model(inputs) # 순전파\n",
    "    loss = criterion(outputs, labels)   # 손실 계산\n",
    "    print(loss.item())\n",
    "\n",
    "    optimizer.zero_grad()   # 이전 단계에서 계산된 기울기를 0으로 초기화\n",
    "    loss.backward()         # 손실에 대한 역전파 수행 (파라미터에 대한 기울기 계산)\n",
    "    optimizer.step()        # 계산된 기울기를 사용하여 옵티마이저가 모델의 파라미터 업데이트\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
